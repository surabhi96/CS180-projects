<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Fun with Filters and Frequencies</title>
  <style>
    body { font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }
    h1, h2, h3 { margin-top: 40px; }
    img { max-width: 400px; margin: 10px; border: 1px solid #ccc; }
    .section { margin-bottom: 60px; }
  </style>
</head>
<body>

<h1>Fun with Filters and Frequencies!</h1>
<!-- <p>This project explores 2D convolution, filtering, frequencies, and multi-resolution blending. 
All results shown below were implemented from scratch in Python.</p> -->

<div class="section">
  <h2>Part 1: Fun with Filters</h2>

  <h3>Part 1.1: Convolutions from Scratch!</h3>
<p>
  Implemented convolution with four loops, then two loops (using NumPy elementwise multiplication for the kernel),
  and finally compared against the built-in <code>scipy.signal.convolve2d</code>. Although the results of these convolutions look the same, the compute time varied dramatically. Below are the results shown for a 15x15 convolution which provided a more perceptable blur. 
  Time taken for (4 for loop) custom convolve:  573.5813066959381 seconds
  Time taken for (2 for loop) custom convolve:  30.86306118965149 seconds
  Time taken for signal.convolve2d convolve:  3.6357760429382324 seconds
</p>

<h4>Custom Convolution (4 loops)</h4>
<pre><code>
def convolve_2d(im, kernal):
    im_h, im_w = im.shape
    k_h, k_w = kernal.shape
    p_h, p_w = k_h // 2, k_w // 2   

    padded = np.pad(im, ((p_h, p_h), (p_w, p_w)), mode="constant", constant_values=0)
    output_im = np.zeros_like(im, dtype=np.float32)

    for y in range(0, im_h):
        for x in range(0, im_w):
            res = 0.0
            for i in range(0, k_h):
                for j in range(0, k_w):
                    res += kernal[i,j] * padded[y+i,x+j]
            output_im[y,x] = res
    return output_im
</code></pre>

<h4>Custom Convolution (2 loops)</h4>
<pre><code>
def convolve_2d_optim(im, kernal):
    im_h, im_w = im.shape
    k_h, k_w = kernal.shape
    p_h, p_w = k_h // 2, k_w // 2   

    padded = np.pad(im, ((p_h, p_h), (p_w, p_w)), mode="constant", constant_values=0)
    output_im = np.zeros_like(im, dtype=np.float32)

    for y in range(0, im_h):
        for x in range(0, im_w):
            window = padded[y:y+k_h, x:x+k_w]
            output_im[y,x] = np.sum(kernal * window)
    return output_im
</code></pre>

<p>Below is the result of applying a 15x15 box filter using each method.</p>
<img src="results/blur.jpeg" alt="box filter convolution">

  <h3>Part 1.2: Finite Difference Operator</h3>
  <p>Computed partial derivatives in X and Y of the cameraman image using Dx and Dy respectively, which was then used to compute gradient magnitude and binarized edge map.</p>
  <img src="results/cameraman_grad.jpeg" alt="Gradient magnitude" style="width:100%;>

  <h3>Part 1.3: Derivative of Gaussian (DoG) Filter</h3>
  <p>Smoothed the image with a Gaussian filter, then computed derivatives using DoG filters. The improvements over the result of 1.2 include removal of high frequency noise using gaussian before convolution which results in smoother/prominent edges and less noise. Note: the evaluations above and below are done with a threshold of 50 pixels to create the binary edge map.</p>
  <img src="results/dog_filter.jpeg" alt="DoG filters" width=500 height="250">
  <img src="results/dog_filtered_image.jpeg" alt="Gradient magnitude" width="1000" height="250">

</div>

<div class="section">
  <h2>Part 2: Fun with Frequencies!</h2>

  <h3>Part 2.1: Image Sharpening</h3>
  <p>Implemented unsharp masking to sharpen images by enhancing their high frequencies.</p>
  <img src="results/taj.png" alt="Taj sharpened">
  <p>
  To evaluate the sharpening approach, the original image was blurred with a Gaussian filter, 
  and then sharpened again. The sharpening step enhances the blurred image slightly, but it cannot fully recover 
  the original fine details that were lost during blurring. Shapening only amplifies the edges and sometimes might .
</p>

<div style="display:flex; gap:15px; justify-content:center;">
  <figure>
    <img src="data/nutmeg.jpg" alt="Nutmeg original" width="250" height="250">
    <figcaption>Original</figcaption>
  </figure>
  <figure>
    <img src="results/nutmeg_sharpen.png" alt="Nutmeg sharpened" width="250" height="250">
    <figcaption>Sharpened (after blur)</figcaption>
  </figure>
</div>

  <h3>Part 2.2: Hybrid Images</h3>
  <p>By combining low frequencies of one image and high frequencies of another, we created hybrid images.</p>
  <img src="results/teaser.png" alt="Hybrid teaser">

  <h3>Part 2.3: Gaussian and Laplacian Stacks</h3>
  <p>We implemented Gaussian and Laplacian stacks for multi-resolution blending.</p>
  <img src="results/lincoln.png" alt="Stack example">

  <h3>Part 2.4: Multiresolution Blending (The Oraple)</h3>
  <p>We blended an apple and an orange seamlessly using Laplacian stacks and Gaussian mask stacks.</p>
  <img src="results/oraple.png" alt="Oraple result">
</div>

</body>
</html>
